{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8f5422",
   "metadata": {},
   "source": [
    "# N Gram feature selection based LSTM model for fake newsÂ detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d22488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\surya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 200)\n",
      "(7613, 200)\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "(7613, 1)\n",
      "(7613, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "from tkinter import messagebox\n",
    "from tkinter import *\n",
    "from tkinter import simpledialog\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation,Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras.layers\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "\n",
    "main = Tk()\n",
    "main.title(\"DETECTION OF FAKE NEWS THROUGH IMPLEMENTATION OF DATA SCIENCE APPLICATION\")\n",
    "main.geometry(\"1300x1200\")\n",
    "\n",
    "global filename\n",
    "global X, Y\n",
    "global tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test\n",
    "global tfidf_vectorizer\n",
    "global accuracy,error\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "textdata = []\n",
    "labels = []\n",
    "global classifier\n",
    "\n",
    "\n",
    "def cleanPost(doc):\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def uploadDataset():    \n",
    "    global filename\n",
    "    text.delete('1.0', END)\n",
    "    filename = filedialog.askopenfilename(initialdir=\"TwitterNewsData\")\n",
    "    textdata.clear()\n",
    "    labels.clear()\n",
    "    dataset = pd.read_csv(filename)\n",
    "    dataset = dataset.fillna(' ')\n",
    "    for i in range(len(dataset)):\n",
    "        msg = dataset._get_value(i, 'text')\n",
    "        label = dataset._get_value(i, 'target')\n",
    "        msg = str(msg)\n",
    "        msg = msg.strip().lower()\n",
    "        labels.append(int(label))\n",
    "        clean = cleanPost(msg)\n",
    "        textdata.append(clean)\n",
    "        text.insert(END,clean+\" ==== \"+str(label)+\"\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "def preprocess():\n",
    "    text.delete('1.0', END)\n",
    "    global X, Y\n",
    "    global tfidf_vectorizer\n",
    "    global tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test\n",
    "    stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords, use_idf=True, ngram_range=(1,2),smooth_idf=False, norm=None, decode_error='replace', max_features=200)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(textdata).toarray()        \n",
    "    df = pd.DataFrame(tfidf, columns=tfidf_vectorizer.get_feature_names())\n",
    "    text.insert(END,str(df))\n",
    "    print(df.shape)\n",
    "    df = df.values\n",
    "    X = df[:, 0:df.shape[1]]\n",
    "    X = normalize(X)\n",
    "    Y = np.asarray(labels)\n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(Y)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    Y = Y[indices]\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    print(X.shape)\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    #Y = encoder.fit_transform(Y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    print(Y)\n",
    "    print(Y.shape)\n",
    "    print(X.shape)\n",
    "    tfidf_X_train, tfidf_X_test, tfidf_y_train, tfidf_y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    text.insert(END,\"\\n\\nTotal News found in dataset : \"+str(len(X))+\"\\n\")\n",
    "    text.insert(END,\"Total records used to train machine learning algorithms : \"+str(len(tfidf_X_train))+\"\\n\")\n",
    "    text.insert(END,\"Total records used to test machine learning algorithms  : \"+str(len(tfidf_X_test))+\"\\n\")\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "font = ('times', 15, 'bold')\n",
    "title = Label(main, text='DETECTION OF FAKE NEWS THROUGH IMPLEMENTATION OF DATA SCIENCE APPLICATION')\n",
    "title.config(bg='gold2', fg='thistle1')  \n",
    "title.config(font=font)           \n",
    "title.config(height=3, width=120)       \n",
    "title.place(x=0,y=5)\n",
    "\n",
    "font1 = ('times', 13, 'bold')\n",
    "ff = ('times', 12, 'bold')\n",
    "\n",
    "uploadButton = Button(main, text=\"Upload Fake News Dataset\", command=uploadDataset)\n",
    "uploadButton.place(x=20,y=100)\n",
    "uploadButton.config(font=ff)\n",
    "\n",
    "\n",
    "processButton = Button(main, text=\"Preprocess Dataset\", command=preprocess)\n",
    "processButton.place(x=20,y=150)\n",
    "processButton.config(font=ff)\n",
    "\n",
    "font1 = ('times', 12, 'bold')\n",
    "text=Text(main,height=30,width=100)\n",
    "scroll=Scrollbar(text)\n",
    "text.configure(yscrollcommand=scroll.set)\n",
    "text.place(x=330,y=100)\n",
    "text.config(font=font1)\n",
    "\n",
    "main.config(bg='DarkSlateGray1')\n",
    "main.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be8bdef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16de82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
